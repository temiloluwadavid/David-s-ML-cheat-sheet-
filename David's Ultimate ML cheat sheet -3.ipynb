{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#pre-processing \n",
    "#Data preprocessing is an important step in the data mining process. \n",
    "#This is particularly applicable to data mining and machine learning projects.\n",
    "#Data-gathering methods are often loosely controlled, resulting in out-of-range values, impossible data combinations, missing values, etc\n",
    "#about 70% of your time building an ML algorithm would be spent doing data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "# you would still have to import more libraries when building an ML model e.g SKlearn,model_selection,XGboost,NLP.\n",
    "#the models you would import varies with the task. however, the libraries imported baove is sufficient to preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dataset \n",
    "dataset = pd.read_csv('Data.csv') #pd.read_csv is from the pandas library\n",
    "#investigate which column has more missing value\n",
    "nulls = dataset.isnull().sum()[dataset.isnull().sum() > 0].sort_values(ascending=False).to_frame().rename(columns={0: \"MissingVals\"})\n",
    "nulls[\"MissingValsPct\"] = nulls[\"MissingVals\"] / len(train)\n",
    "nulls\n",
    "#splitting into x and y\n",
    "x = dataset.iloc[:,].values# here you assigning the independent valriable/s to x \n",
    "y = dataset.iloc[:,].values# here you assigning the dependent valriable/s to y\n",
    "#before the comma means all the rows and after the comma you select with column you want to work with. \n",
    "#you might have to change the import fuction depending on the data type you are importing.\n",
    "# you can always add more conditionsbefore importing e.g delimiter ='\\t' for tsv files(this type of file import is used during NLP)\n",
    "# you can always check more condition by pressing shift + tab in jupyter note book or CMD/CTRL + i on spyder\n",
    "#N/B indexing in python starts from 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dealing with missing data\n",
    "#1. you can drop it\n",
    "#2. take the mean of columns \n",
    "#3. take the median of columns\n",
    "#4. take the most frequent value \n",
    "x = x.dropna()#to drop missing data \n",
    "\n",
    "from sklearn.preprocessing import Imputer \n",
    "imputer = Imputer(missing_values='NaN',strategy = 'mean', axis = 0)#to use the mean of the column\n",
    "imputer = imputer.fit(x[:,])#fitting th eimputer to the data\n",
    "x[:,] = imputer.transform(x[:,])#transforming the data\n",
    "#N/B:before the comma means all the rows and after the comma you select with column you want to work with. \n",
    "\n",
    "imputer = Imputer(missing_values='NaN',strategy = 'median', axis = 0)#to take the median of the column\n",
    "imputer = imputer.fit(x[:,])#fitting th eimputer to the data\n",
    "x[:,] = imputer.transform(x[:,])#transforming the data \n",
    "#N/B:before the comma means all the rows and after the comma you select with column you want to work with. \n",
    "\n",
    "imputer = Imputer(missing_values='NaN',strategy = 'most_frequent', axis = 0)#to take the most frequent/mode of the column\n",
    "imputer = imputer.fit(x[:,]) #fitting th eimputer to the data\n",
    "x[:,] = imputer.transform(x[:,])#transforming the data \n",
    "#N/B:before the comma means all the rows and after the comma you select with column you want to work with. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding categorical data \n",
    "#1.labelEncoder \n",
    "#2.OneHotEncoder\n",
    "\n",
    "#label encoder\n",
    "g= x.iloc[:,]# in this case g is the column that we want to encode \n",
    "from sklearn import preprocessing \n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(g)\n",
    "g=le.transform(g)\n",
    "x['column _name']=g\n",
    "#however, the machine might think a value is more important than the other which is not true in most cases so  using a onehotencoder might be better.\n",
    "\n",
    "#one hot encoder \n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder() # created an object for the function \n",
    "x[:,] = labelencoder_X.fit_transform(x[:,])# select the column you want to encode\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "x = onehotencoder.fit_transform(x).toarray()\n",
    "\n",
    "#dummy varibale trap\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into test and train\n",
    "from sklearn.model_selection import train_test_split #importing the library \n",
    "x_train,x_test,y_train,y_test = test_train_split(x, y, test_size=0.2, random_state=0)#splitting: test:20% train:80%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#feature Scaling \n",
    "Feature scaling is a method used to normalize the range of independent variables or features of data. \n",
    "In data processing, it is also known as data normalization and is generally performed during the data preprocessing step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler#importing the library \n",
    "sc_X = StandardScaler()#creating an object\n",
    "x_train = sc_X.fit_transform(x_train)\n",
    "x_test = sc_X.transform(x_test)\n",
    "sc_y = StandardScaler()#creating an object\n",
    "y_train = sc_y.fit_transform(y_train)\n",
    "#its not used everytime but its still something to have in your ML arsenal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression \n",
    "1.Linear regression \n",
    "2.Multilinear regression \n",
    "3.polynomial regression \n",
    "4.support vector regression \n",
    "5.decision tree regression \n",
    "6. random forest regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression \n",
    "from sklearn.linear_model import LinearRegression\n",
    "MLR = LinearRegression()\n",
    "MLR.fit(x_train,y_train)\n",
    "y_pred = MLR.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward elimination (or backward deletion) is the reverse process. \n",
    "All the independent variables are entered into the equation first and each one is deleted one at a time if they do not contribute to the regression equation. \n",
    "Stepwise selection is considered a variation of the previous two methods\n",
    "step 1 : Select a significant level \n",
    "2: fit the model with all the predictors\n",
    "3:consider the predictor with the highest value. if the value is greater tahn the initial significant level, remove it if not your have gotten the optimal amount of independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MultiLinear Regression \n",
    "from sklearn.linear_model import LinearRegression\n",
    "MLR = LinearRegression()\n",
    "MLR.fit(x_train,y_train)\n",
    "y_pred = MLR.predict(x_test)\n",
    "# the difference between a multilinear regression and the regular linear regression is beacuse we would be using a backward elimination method.\n",
    "#the bacward elimination method is used to remove columns of less importnace to increase the accuracy of the model.\n",
    "# Building the optimal model using Backward Elimination\n",
    "import statsmodels.formula.api as sm\n",
    "x = np.append(arr = np.ones((, )).astype(int), values = x, axis = 1)#adding a columns of ones \n",
    "x_opt = x[:, []]# the indexes of the columns should be in the []\n",
    "MLR_OLS = sm.OLS(endog = y, exog = x_opt).fit()\n",
    "MLR_OLS.summary()\n",
    "x_opt = x[:, []]\n",
    "MLR_OLS = sm.OLS(endog = y, exog = x_opt).fit()\n",
    "MLR_OLS.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polynomial Regression\n",
    "#from sklearn.preprocessing import PolynomialFeatures\n",
    "#poly_reg = PolynomialFeatures(degree = 4)\n",
    "#X_poly = poly_reg.fit_transform(X)\n",
    "#poly_reg.fit(X_poly, y)\n",
    "#lin_reg_2 = LinearRegression()\n",
    "#lin_reg_2.fit(X_poly, y)\n",
    "#still work in progress "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVR\n",
    "from sklearn.svm import SVR\n",
    "SVR = SVR(kernel = 'rbf')\n",
    "SVR.fit(x_train,y_train)\n",
    "y_pred = SVR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision Tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "DT = DecisionTreeRegressor(random_state = 0)\n",
    "DT.fit(x_train, y_train)\n",
    "y_pred = SVR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RF = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "RF.fit(x_train, y_train)\n",
    "y_pred = SVR.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group are more similar to each other than to those in other groups.\n",
    "K-means \n",
    "HC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kmeans\n",
    "# Using the elbow method to find the optimal number of clusters\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "kmeans = KMeans(n_clusters = , init = 'k-means++', random_state = 42)#n_clusters should be from the elbow rule visulaization\n",
    "y_kmeans = kmeans.fit_predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HC\n",
    "# Using the dendrogram to find the optimal number of clusters\n",
    "import scipy.cluster.hierarchy as sch\n",
    "dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.show()\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "hc = AgglomerativeClustering(n_clusters = , affinity = 'euclidean', linkage = 'ward')#n_clusters hsould be from the dendogram visulaization\n",
    "y_hc = hc.fit_predict(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classification\n",
    "Classification is the process of predicting the class of given data points. Classes are sometimes called as targets/ labels or categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(x_train,y_train)\n",
    "predictions = knn.predict(x_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "NB = GaussianNB()\n",
    "NB.fit(x_train,y_train)\n",
    "prediction = NB.predict(x_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DT\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "DT = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "DT.fit(x_train,y_train)\n",
    "predictions =DT.predict(x_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "from sklearn.svm import SVC\n",
    "SVM = SVC(kernel='linear',random_state =0)\n",
    "SVM.fit(x_train,y_train)\n",
    "predictions =SVM.predict(x_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "#non linear SVM\n",
    "from sklearn.svm import SVC\n",
    "SVM = SVC(kernel='rbf',random_state =0)\n",
    "SVM.fit(x_train,y_train)\n",
    "predictions =SVM.predict(x_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "#There are other forms of non linear SVM like sigmoid and polynomial you just change kernal type to use it \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Classification \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "RF.fit(x_train, y_train)\n",
    "predictions =RF.predict(x_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation \n",
    "Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 10) #estimator = classifer is the classifer used to build your model\n",
    "accuracies.mean()\n",
    "accuracies.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search\n",
    "Grid-search is a way to select the best of a family of models, parametrized by a grid of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "              {'C': [1, 10, 100, 1000], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\n",
    "grid_search = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10,\n",
    "                           n_jobs = -1)\n",
    "grid_search = grid_search.fit(x_train, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
